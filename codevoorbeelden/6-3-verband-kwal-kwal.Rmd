---
title: "6.3 -- Verband tussen twee kwalitatieve variabelen"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Als casus lezen we resultaten in van een enquête uitgevoerd onder de klanten van een restaurant aan een hogeschool.

```{r, message=FALSE}
library(foreign)
resto <- read.spss("../cursus/data/catering_hogeschool.sav",
                   to.data.frame = TRUE)
```

## Chi-kwadraat en Cramér's V

Chi-kwadraat en Cramér's V zijn maten om te bepalen of er een verband bestaat tussen twee kwalitatieve variabelen (in R: factors).

Stel, we wensen te bepalen of vrouwen en mannen (variabele `Geslacht`) een andere mening hebben over de keuzemogelijkheden in het basisassortiment (variabele `Keuze_basis`). Als dat zo is, dan zeggen we dat er een verband is tussen de variabelen `Geslacht` en `Keuze_basis`. Als de antwoorden tussen beide groepen (ongeveer) gelijk zijn, dan zeggen we dat er *geen* verband is.

De resultaten van de enquête zijn:

```{r}
# Bereken een frequentietabel. Eerst de afhankelijke, vervolgens
# de onafhankelijke variabele.
observed <- table(resto$Keuze_basis,
                  resto$Geslacht)
# Voeg rij- en kolomtotalen toe aan de tabel
addmargins(observed)
```

### Uitgewerkte berekening

Een eerste stap om na te gaan of er verschillen zijn is om eerst na te gaan wat de waarden in deze frequentietabel zouden moeten zijn als vrouwen en mannen op een gelijkaardige manier antwoorden op de vraag. In dat geval zou je een tabel moeten krijgen met dezelfde rij- en kolomtotalen, maar gelijkmatig verdeeld. Deze aantallen verkrijg je door in elke cel het product te nemen van het rij- en kolomtotaal en te delen door het totaal aantal respondenten. Bijvoorbeeld voor de cel linksboven (vrouwen die "Goed" antwoordden) krijg je `22 * 17 / 49`. De gehele tabel bereken je dan zo:

```{r}
row_sums <- rowSums(observed)              # rijtotalen
col_sums <- colSums(observed)              # kolomtotalen
n <- sum(observed)                         # totaal hele tabel
expected <- outer(row_sums, col_sums) / n  # verwachte waarden
addmargins(expected)                       # voeg totalen toe
```

Zoals je kan zien zijn de rij- en kolomtotalen inderdaad gelijk aan deze van de geobserveerde waarden. Wat is nu het verschil tussen beide?

```{r}
expected - observed
```

Sommige waarden lijken sterker af te wijken (bv. "Goed"), anderen veel minder (bv. "Voldoende"). Een maat om de totale afwijking in een frequentietabel te bepalen, bestaat er uit om de verschillen tussen verwachte en geobserveerde waarden te kwadrateren (net zoals men bij variantie/standaardafwijking doet) en te delen door de verwachte waarde:

```{r}
diffs <- (expected - observed)^2 / expected
diffs
```

De som van al deze waarden wordt $\chi^2$ ("chi-kwadraat") genoemd.

```{r}
chi_squared <- sum(diffs)
chi_squared
```

Nu zegt deze waarde op zich nog steeds niet zo veel. Onder welke voorwaarden zeggen we dat er al dan niet een verband is tussen beide variabelen? Een en ander zal ook afhangen van de grootte van de tabel en het totaal aantal observaties. In een kruistabel met meer rijen/kolommen, zal je een grotere $\chi^2$ moeten hebben om te besluiten dat er een verband is.

*Cramér's V* is een formule waarmee de $\chi^2$ kan genormaliseerd worden tot een waarde tussen 0 en 1 die onafhankelijk is van de tabelgrootte:

```{r}
k <- min(nrow(observed), ncol(observed))
cramers_v <- sqrt(chi_squared / ((k - 1) * n))
cramers_v
```

Om een besluit te trekken uit dit getal, vergelijk je het met de waarden in onderstaande tabel:

| Cramér's V | Besluit                |
| :---:      | :---                   |
| 0          | Geen verband           |
| 0.1        | Zwak verband           |
| 0.25       | Redelijk sterk verband |
| 0.50       | Sterk verband          |
| 0.75       | Zeer sterk verband     |
| 1          | Volledig verband       |

We kunnen dus besluiten dat er een redelijk sterk verband bestaat tussen de variabelen `Geslacht` en `Keuze_basis`.

### R-functies

In R zijn er al functies geschreven voor de berekening van $\chi^2$ en Cramér's V. Je hoeft dus niet telkens de berekeningen van hierboven te herhalen, maar kan meteen de juiste functie gebruiken, meer bepaald `assocstats` uit de library `vcd`:

```{r}
library(vcd)
assocstats(observed)
```

Deze functie geeft zowel de waarde van "Pearson's $\chi^2$" (derde regel, eerste cijfer) als Cramér's V (laatste regel). Beide komen overeen met de waarden die we hierboven berekend hebben.

## The chi-squared test

The figure below shows the density function of the $\chi^2$-distribution for several degrees of freedom.

```{r}
x <- seq(0, 8, length=100)  # x-values
degf <- c(1, 2, 3, 5, 10)   # degrees of freedom to be plotted

# line colors and legend labels
colors <- c("black", "red", "blue", "darkgreen", "gold")
labels <- c("df=1", "df=2", "df=3", "df=5", "df=10")

# plot of the distribution with df = 1
plot(x, dchisq(x, degf[1]),
     col = colors[1],
     type = "l", lwd = 2,
     ylim = c(0, .5),
     xlab = "chi-squared",
     ylab = "density",
     main = "Comparison of chi-squared distributions")

# plots of the distribution with higher degrees of freedom
for (i in 2:5){
  lines(x, dchisq(x, degf[i]), lwd=2, col=colors[i])
}

legend("topright", inset=.05, title="Verdelingen",
       labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors,
       cex = .5)
```

R has several functions for calculations related to the $\chi^2$ density function, just like those for the normal and $t$ distributions.

- `dchisq(x, df)`: the density function
- `pchisq(x, df)`: the left-tail probability $P(\chi^2 < x)$
- `qchisq(p, df)`: the inverse of `pchisq`, "find a number $x$ so that $P(\chi^2 < x) = p$"
- `rchisq(n, df)`: generate $n$ random numbers for a $\chi^2$ distribution

The parameter `df` denotes the degrees of freedom. The value depends on the contingency table. In the case of a goodness of fit test (see below), `df` is the number of cells minus one. In the case of a chi-squared test for independence of categorical data, `df` is the product of the number of rows ($r$) minus one and the number of columns ($l$) minus one: $df = (r - 1) (l - 1)$.

### Goodness of fit test

In a sample of super heroes, the following types were observed:

```{r}
types      <- c("mutant", "human", "alien", "god", "demon")
observed   <- c(   127,      75,      98,     27,     73)
expected_p <- c(   .35,     .17,     .23,    .08,    .17)
```

The question now is, is this sample representative for the population w.r.t. the different types? I.e. does each type of super hero occur in the sample proportional to the expected percentages in the population as a whole?

#### Test procedure

The *goodness of fit test* is suitable to answer this type of question. The procedure is as follows:

1. Formulate the hypotheses:
    - $H_0$: the sample is representative for the population (i.e. the proportions of each class in the sample closely matches those of the population)
    - $H_1$: the sample is **not** representative for the population (i.e. the proportions diverge *significantly*)
2. Determine a significance level, e.g. $\alpha = 0.05$ and the sample size

    ```{r}
    alpha <- 0.05
    n <- sum(observed)
    expected <- n * expected_p
    expected
    ```

3. Calculate the test statistic, in this case $\chi^2$:

    ```{r}
    chisq <- sum((observed - expected)^2 / expected)
    chisq
    ```

4. Determine the $p$-value or the critical value $g$.  Remark that in practice, you only need to calculate **one** of the two. Both methods are equivalent.

    a. In a $\chi^2$-test, the critical value is a number $g$ with property $P(\chi^2 > g) = \alpha$ (where $\alpha$ is our chosen significance level). *Left* of $g$ is the acceptance region, *right* of $g$ the critical region (see the plot below). This number can be calculated with:

        ```{r}
        k <- length(types)
        g <- qchisq(p = 1 - alpha, df = k - 1)
        g
        ```
    
    b. The $p$-value is given by:
    
        ```{r}
        p <- 1 - pchisq(chisq, df = k - 1)
        p
        ```
 
 5. Draw a conclusion:
 
    a. In the case of the critical value $g$:
        - if $\chi^2 < g$, accept the null hypothesis,
        - if $\chi^2 > g$, reject the null hypothesis
    b. In the case of the $p$-value:
        - if $p > \alpha$, accept the null hypothesis,
        - if $p < \alpha$, reject the null hypothesis
        
    ```{r}
    # Critical value $g$
    paste(ifelse(chisq < g, "Accept", "Reject"), "the null hypothesis")
    # Probability value $p$
    paste(ifelse(p > alpha, "Accept", "Reject"), "the null hypothesis")
    ```

#### Plot of this case

```{r}
# Plot the chi-squared density function
x <- seq(0, 15, length = 100)
dist <- dchisq(x, df = k - 1)
plot (x, dist, type = 'l', xlab = '', ylab = '')

# The acceptance region (where H_0 is accepted)
i <- x <= g
polygon(c(x[i],    g,                     g),
        c(dist[i], dchisq(g, df = k - 1), 0),
        col = 'lightgreen')
text(x = 4, y = 0.05, 'Acceptance region (H0)')
text(x = g, y = 0.01, paste('g = ', round(g, digits=2)))

# The test statistic (chi squared)
abline(v = chisq, col = 'red')
text(x = chisq, y = 0.15, paste('chi^2 = ', round(chisq, digits=2)))

```

#### Using the R function `chisq.test`

The `chisq.test` function automates the entire process. You can provide it with the observed values in the sample and the expected proportions in the population, and it will perform all calculations. The most important part of the output is the $p$-value.

```{r}
chisq_test_result <- chisq.test(observed, p = expected_p)
chisq_test_result
```

It's useful to assign the result of the `chisq.test` function to a variable, since it contains information that isn't printed:

```{r}
chisq_test_result$statistic # The value of chi squared
chisq_test_result$p.value   # The p-value
chisq_test_result$parameter # The degrees of freedom
chisq_test_result$residuals # Residuals (o - e) / sqrt(e)
chisq_test_result$stdres    # Standardised residuals
```

The standardised residuals are noteworthy: they indicate the categories that are over- or underrepresented in the sample. If the value is between $[-2, 2]$, the category is considered to be well represented in the sample. Below -2, it's underrepresented, above 2, it is overrepresented.

### Another example

```{r, include=FALSE}
rm(list = ls())
```

In some research project, 1022 families with (exactly) five children are selected in a sample. The families are categorised according to the number of boys. Below, the frequencies for each category are given:

```{r}
num_boys <- c( 0,   1,   2,   3,   4,  5)
observed <- c(58, 149, 305, 303, 162, 45)
n <- sum(observed)
```

The expected number of boys (assuming the probability of conceiving either a boy or a girl is 50%) can be calculated as follows:

```{r}
prob_boy <- 0.5

expected_p <- choose(n = 5, k = num_boys) * 
  prob_boy^num_boys * 
  prob_boy^(5-num_boys)
expected_p
expected <- expected_p * n
expected
```

The test procedure:

1. $H_0$: the sample is representative; $H_1$: it is *not* representative
2. Choose significance level:

    ```{r}
    alpha <- 0.01
    ```
    
3. Calculate the test statistic
    
    ```{r}
    chisq <- sum((observed - expected)^2 / expected)
    chisq
    ```
    
4. Calculate $g$ or $p$

    ```{r}
    k <- length(num_boys)
    g <- qchisq(p = 1 - alpha, df = k - 1)
    g
    p <- 1 - pchisq(chisq, df = k - 1)
    p
    ```
    
5. Draw a conclusion:

    ```{r}
    # Critical value $g$
    paste(ifelse(chisq < g, "Accept", "Reject"), "the null hypothesis")
    # Probability value $p$
    paste(ifelse(p > alpha, "Accept", "Reject"), "the null hypothesis")
    ```

In order to find out which categories of families are under- or overrepresented, take a look at the standardized residuals:

```{r}
stres <- (observed - n * expected_p) /
  sqrt(n * expected_p * (1 - expected_p))
stres
```

The standardised residuals for the families with either only girls (~4.69) or only boys (~2.35) are outside of the interval [-2, 2]. Consequently, these families are overrepresented in the sample.

#### Plot of this case

```{r}
# Plot the chi-squared density function
x <- seq(0, 35, length = 100)
dist <- dchisq(x, df = k - 1)
plot (x, dist, type = 'l', xlab = '', ylab = '')

# The acceptance region (where H_0 is accepted)
i <- x <= g
polygon(c(x[i],    g,                     g),
        c(dist[i], dchisq(g, df = k - 1), 0),
        col = 'lightgreen')
abline(v = g)
text(x = 4, y = 0.05, '(H0)')
text(x = g, y = 0.02, paste('g = ', round(g, digits=2)))

# The test statistic (chi squared)
abline(v = chisq, col = 'red')
text(x = chisq, y = 0.1, paste('chi^2 = ', round(chisq, digits=2)))

```

## Chi-squared test for independence of categorical data

With this variant of the Chi-squared test, we can determine whether two categorical (i.e. qualitative) variables are associated.

The null hypothesis in this case is that the two variables are independent, i.e. all columns in the contingency table have similar distributions of frequencies. The alternative hypothesis is that there is an association between the two variables.

### Example

We'll take the `MASS::survey` dataset as an example and will try to determine if variables `Smoke` and `Exer` are associated.

```{r}
library(MASS)
exer_smoke <- table(survey$Smoke, survey$Exer)
plot(exer_smoke)
exer_smoke_test <- chisq.test(exer_smoke)
exer_smoke_test
```

Since the p-value is approximately 0.483, the null hypothesis cannot be rejected. So, there is no association between smoking habits and exercise. Or, smokers do not have significantly different exercise habits from non-smokers.

Remark that the degrees of freedom is 6, i.e.:

```{r}
df_exer_smoke <- (nrow(exer_smoke) - 1) * (ncol(exer_smoke) - 1)
df_exer_smoke
```

### Cochran's rule

A chi-squared test only gives good results if you have enough observations in each category. A rule of thumb for what exactly is *enough* was formulated by Cochran, and applies to contingency tables larger than 2x2:

* All expected values should be at least 1
* At most 20% of the expected values should be below 5

Let's check whether the previous case satisfies Cochran's rule.

```{r}
# Function that checks whether the specified contingency table x
# satisfies Cochran's rule.
cochrans_rule <- function(x) {
  test <- chisq.test(x)
  EXPECTED = test$expected
  
  # Rule 1. None of the expected values should be less than 1
  EXP_LT_1 = table(EXPECTED < 1)
  NUM_LT_1 = as.numeric(EXP_LT_1['TRUE'])
  if(is.na(NUM_LT_1)) {
    NUM_LT_1 = 0
  }
  SAT_LT_1 = NUM_LT_1 == 0
  
  # Rule 2. At most 20% of expected values should be less than 5
  EXP_LT_5 = table(EXPECTED < 5)
  PCT_LT_5 = as.numeric(EXP_LT_5['TRUE']) / sum(EXP_LT_5)
  if(is.na(PCT_LT_5)) {
    PCT_LT_5 = 0
  }
  SAT_LT_5 = PCT_LT_5 < .2
  
  # Return final result and intermediate steps
  structure(list(
    satisfied = SAT_LT_1 & SAT_LT_5,
    
    expected = EXPECTED,
    
    expected.lt.1 = EXP_LT_1,
    satisfied.lt.1 = SAT_LT_1,
    
    expected.lt.5 = EXP_LT_5,
    satisfied.lt.5 = SAT_LT_5
  ))
}

cochrans_rule(exer_smoke)
```

So, Cochran's rule was NOT satisfied. Specifically, 4 out of 12 expected values are less than 5, or 33%.

### Another example

As another example, we take a study by Doll and Hill, who in 1951 conducted a survey among British general practicioners with the request for data about their age and whether they smoked or not. They then followed up on the respondents for years and recorded whether they died of lung cancer or not.

The results of the survey are given in the following table:

```{r}
doll_hill <- matrix(data = c(21178, 83, 3092, 1),
                    ncol = 2,
                    byrow = TRUE,
                    dimnames = list(c("Smoker", "Non-smoker"),
                                 c("No Cancer", "Cancer")))
doll_hill
```

The research question now is: "is there an association with smoking and dying of lung cancer?"

```{r}
doll_hill_result <- chisq.test(doll_hill)
doll_hill_result
```

```{r}
doll_hill_result$expected
doll_hill_result$stdres
```
